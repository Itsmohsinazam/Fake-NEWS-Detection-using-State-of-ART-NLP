# -*- coding: utf-8 -*-
"""Multilingual BERT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FUdJmXvlTFJeIz7XOaG6I2i3HkMcIPMV
"""

!pip uninstall -y transformers datasets numpy
!pip install transformers==4.28.1 datasets==2.10.1 numpy==1.26.4 scikit-learn --quiet

"""# ***Mount Google Drive & Load CSV Files***"""

import pandas as pd

train_path = "/kaggle/input/fake-news-data/Train_1.csv"
test_path  = "/kaggle/input/fake-news-data/Test1.csv"
valid_path = "/kaggle/input/fake-news-data/Valid1.csv"

train_df = pd.read_csv(train_path)
test_df = pd.read_csv(test_path)
val_df = pd.read_csv(valid_path)

train_df = train_df[["text", "label"]]
val_df = val_df[["text", "label"]]
test_df = test_df[["text", "label"]]

train_df.shape

train_df.head()

test_df.head()

"""# ***Tokenize Using mBERT***"""

from transformers import BertTokenizer
from datasets import Dataset

tokenizer = BertTokenizer.from_pretrained("bert-base-multilingual-cased")

train_ds = Dataset.from_pandas(train_df)
val_ds = Dataset.from_pandas(val_df)
test_ds = Dataset.from_pandas(test_df)

def tokenize_fn(example):
    return tokenizer(example["text"], truncation=True, padding="max_length", max_length=128)

train_ds = train_ds.map(tokenize_fn, batched=True)
val_ds = val_ds.map(tokenize_fn, batched=True)
test_ds = test_ds.map(tokenize_fn, batched=True)

train_ds.set_format(type="torch", columns=["input_ids", "attention_mask", "label"])
val_ds.set_format(type="torch", columns=["input_ids", "attention_mask", "label"])
test_ds.set_format(type="torch", columns=["input_ids", "attention_mask", "label"])

"""# ***Train mBERT Using Trainer***"""

import os
import warnings
warnings.filterwarnings("ignore", message="Unable to avoid copy*")
os.environ["WANDB_DISABLED"] = "true"

from transformers import BertForSequenceClassification, Trainer, TrainingArguments, DataCollatorWithPadding
from sklearn.metrics import accuracy_score, precision_recall_fscore_support

"""# ***Load Model & Define Metrics***"""

model = BertForSequenceClassification.from_pretrained("bert-base-multilingual-cased", num_labels=3)

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = logits.argmax(axis=1)
    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='macro')
    acc = accuracy_score(labels, predictions)
    return {
        "accuracy": acc,
        "precision": precision,
        "recall": recall,
        "f1": f1
    }

from transformers import TrainingArguments

training_args = TrainingArguments(
    output_dir="./results",
    do_train=True,
    do_eval=True,
    evaluation_strategy="epoch",           # evaluate every epoch ✅
    save_strategy="epoch",                 # save every epoch ✅
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=5,
    weight_decay=0.01,
    logging_dir="./logs",
    load_best_model_at_end=True,          # works now
    metric_for_best_model="f1"
)

data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_ds,
    eval_dataset=val_ds,
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
)

trainer.train()

save_path = "/kaggle/working/fake_news_xlmroberta_model"

# Save model + tokenizer
model.save_pretrained(save_path)
tokenizer.save_pretrained(save_path)

print("✅ Model saved to:", save_path)

"""# ***Results***

# ***1. Predictions on Train DataSet***
"""

from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt
import numpy as np

predictions = trainer.predict(train_ds)
x_true = predictions.label_ids
x_pred = np.argmax(predictions.predictions, axis=1)

cm = confusion_matrix(x_true, x_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=["Fake", "Real"])
disp.plot(cmap="Blues", values_format="d")
plt.title("Confusion Matrix")
plt.show()

print("Classification Report:")
print(classification_report(x_true, x_pred, target_names=["Fake", "Real"]))

"""# **Test_Data**"""

from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt
import numpy as np

predictions = trainer.predict(test_ds)
y_true = predictions.label_ids
y_pred = np.argmax(predictions.predictions, axis=1)

cm = confusion_matrix(y_true, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=["Fake", "Real"])
disp.plot(cmap="Blues", values_format="d")
plt.title("Confusion Matrix")
plt.show()

print("Classification Report:")
print(classification_report(y_true, y_pred, target_names=["Fake", "Real"]))

"""# ***2. ROC Curve & AUC***"""

from sklearn.metrics import roc_curve, auc

y_probs = predictions.predictions[:,1]  # probability for "Real" class
fpr, tpr, _ = roc_curve(y_true, y_probs)
roc_auc = auc(fpr, tpr)

plt.figure()
plt.plot(fpr, tpr, label=f"AUC = {roc_auc:.2f}")
plt.plot([0,1],[0,1], "k--")  # diagonal
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve")
plt.legend()
plt.show()

"""# ***3. Precision-Recall Curve***"""

from sklearn.metrics import precision_recall_curve

prec, rec, _ = precision_recall_curve(y_true, y_probs)

plt.figure()
plt.plot(rec, prec, label="Precision-Recall Curve")
plt.xlabel("Recall")
plt.ylabel("Precision")
plt.title("Precision-Recall Curve")
plt.legend()
plt.show()

"""# ***Learning curves (training vs validation loss / accuracy)***"""

import matplotlib.pyplot as plt
import numpy as np

# extract history (works if trainer logged eval and train losses)
history = trainer.state.log_history  # list of dicts

# parse into per-epoch lists
train_losses = []
eval_losses = []
eval_acc = []
eval_prec = []
eval_rec = []
eval_f1 = []
epochs = []

for record in history:
    # records include step, loss, eval_loss, epoch, eval_accuracy, etc.
    if 'loss' in record and 'epoch' in record:
        # training loss logged at steps; keep last per epoch
        train_losses.append((record['epoch'], record['loss']))
    if 'eval_loss' in record:
        epochs.append(record.get('epoch', len(epochs)+1))
        eval_losses.append(record['eval_loss'])
        # optional metrics (if compute_metrics returns them)
        eval_acc.append(record.get('eval_accuracy', np.nan))
        eval_prec.append(record.get('eval_precision', np.nan))
        eval_rec.append(record.get('eval_recall', np.nan))
        eval_f1.append(record.get('eval_f1', np.nan))

# Convert to arrays aligned by epoch
# For training loss we take last loss value per epoch
if train_losses:
    # dict epoch->loss (keep last logged loss for each epoch)
    ep_train = {}
    for ep, loss in train_losses:
        ep_train[ep] = loss
    train_epochs = sorted(ep_train.keys())
    train_loss_vals = [ep_train[e] for e in train_epochs]
else:
    train_epochs, train_loss_vals = [], []

# Plot losses
plt.figure(figsize=(8,5))
if train_loss_vals:
    plt.plot(train_epochs, train_loss_vals, label='Train Loss', marker='o')
if epochs:
    plt.plot(epochs, eval_losses, label='Validation Loss', marker='o')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training vs Validation Loss')
plt.legend()
plt.grid(True)
plt.show()

# Plot accuracy/F1 if available
if any(np.isfinite(eval_acc)):
    plt.figure(figsize=(8,5))
    plt.plot(epochs, eval_acc, label='Val Accuracy', marker='o')
    if any(np.isfinite(eval_f1)):
        plt.plot(epochs, eval_f1, label='Val F1', marker='o')
    plt.xlabel('Epoch')
    plt.title('Validation Accuracy / F1 per Epoch')
    plt.legend()
    plt.grid(True)
    plt.show()

"""# ***5-Fold Cross-Validation (Hugging Face Trainer)***"""

from sklearn.model_selection import KFold
import pandas as pd
import numpy as np
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer

# Convert dataframe into lists
texts = train_df['text'].tolist()
labels = train_df['label'].tolist()

print("Dataset size:", len(texts))

# Use 5 folds for cross-validation
kf = KFold(n_splits=5, shuffle=True, random_state=42)

# Debug: print sizes for each fold
for fold, (train_idx, val_idx) in enumerate(kf.split(train_df), 1):
    print(f"Fold {fold}: Train size={len(train_idx)}, Val size={len(val_idx)}")

# Pick model + tokenizer
model_name = "distilbert-base-multilingual-cased"
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Dataset wrapper
class HFTextDataset(torch.utils.data.Dataset):
    def __init__(self, texts, labels, tokenizer, max_length=256):
        self.enc = tokenizer(texts, truncation=True, padding="max_length", max_length=max_length)
        self.labels = labels

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        item = {k: torch.tensor(v[idx]) for k, v in self.enc.items()}
        item['labels'] = torch.tensor(self.labels[idx])
        return item

fold_metrics = []  # to store results
fold_idx = 1

import numpy as np
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    preds = np.argmax(logits, axis=-1)
    return {
        "accuracy": accuracy_score(labels, preds),
        "precision": precision_score(labels, preds, average="weighted", zero_division=0),
        "recall": recall_score(labels, preds, average="weighted", zero_division=0),
        "f1": f1_score(labels, preds, average="weighted", zero_division=0),
    }


for train_idx, val_idx in kf.split(texts):
    print(f"\n=== Fold {fold_idx}/5 ===")

    # Split dataset
    train_texts = [texts[i] for i in train_idx]
    train_labels = [labels[i] for i in train_idx]
    val_texts = [texts[i] for i in val_idx]
    val_labels = [labels[i] for i in val_idx]

    print(f"Train size: {len(train_texts)}, Val size: {len(val_texts)}")

    # Build datasets
    train_dataset = HFTextDataset(train_texts, train_labels, tokenizer)
    val_dataset = HFTextDataset(val_texts, val_labels, tokenizer)

    # Reload model fresh each fold
    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)

    # Training arguments
    training_args = TrainingArguments(
        output_dir=f"./cv_results/fold_{fold_idx}",
        evaluation_strategy="epoch",   # fixed: should be `evaluation_strategy`
        save_strategy="no",
        learning_rate=2e-5,
        per_device_train_batch_size=8,
        per_device_eval_batch_size=16,
        num_train_epochs=2,
        weight_decay=0.01,
        logging_dir=f"./cv_logs/fold_{fold_idx}",
        report_to="none"
    )

    # Trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        compute_metrics=compute_metrics
    )

    # Train + evaluate
    trainer.train()
    metrics = trainer.evaluate()

    print(f"Fold {fold_idx} metrics: {metrics}")

    # Save results
    fold_metrics.append({
        "fold": fold_idx,
        "accuracy": metrics.get("eval_accuracy", np.nan),
        "precision": metrics.get("eval_precision", np.nan),
        "recall": metrics.get("eval_recall", np.nan),
        "f1": metrics.get("eval_f1", np.nan)
    })

    fold_idx += 1

df_folds = pd.DataFrame(fold_metrics)

print("\n=== CV Results by Fold ===")
print(df_folds)

print("\n=== Final Mean ± Std ===")
print(df_folds.mean(numeric_only=True))
print(df_folds.std(numeric_only=True))

import matplotlib.pyplot as plt

# Extract fold numbers and metrics
folds = df_folds['fold']
accuracy = df_folds['accuracy']
f1 = df_folds['f1']

# Plot
plt.figure(figsize=(8,5))
bar_width = 0.35
x = np.arange(len(folds))

# Bars for Accuracy & F1
plt.bar(x - bar_width/2, accuracy, width=bar_width, label="Accuracy")
plt.bar(x + bar_width/2, f1, width=bar_width, label="F1 Score")

# Add average lines
plt.axhline(y=accuracy.mean(), color="blue", linestyle="--", label=f"Avg Acc = {accuracy.mean():.4f}")
plt.axhline(y=f1.mean(), color="orange", linestyle="--", label=f"Avg F1 = {f1.mean():.4f}")

# Labels & title
plt.xticks(x, [f"Fold {i}" for i in folds])
plt.ylabel("Score")
plt.ylim(0.95, 1.01)  # zoom in for high scores
plt.title("Cross-Validation Results: Accuracy & F1 per Fold")
plt.legend()
plt.grid(axis="y", linestyle="--", alpha=0.6)

plt.show()

"""# ***Class Distribution (Full Dataset)***"""

# Count samples per class in full dataset
class_counts = train_df['label'].value_counts().sort_index()

# Create a summary table
distribution = pd.DataFrame({
    "Class": ["Fake (0)", "Real (1)"],
    "Count": [class_counts[0], class_counts[1]],
    "Percentage": [class_counts[0]/len(train_df)*100, class_counts[1]/len(train_df)*100]
})

print("\n=== Class Distribution (Full Dataset) ===")
print(distribution)

# If you also want train/val/test split distributions:
from sklearn.model_selection import train_test_split

# Using the deduplicated train_df for splitting
train_df_split, test_df = train_test_split(train_df, test_size=0.2, stratify=train_df['label'], random_state=42)
train_df_split, val_df = train_test_split(train_df_split, test_size=0.1, stratify=train_df_split['label'], random_state=42)

def get_distribution(name, subset):
    counts = subset['label'].value_counts().sort_index()
    return {
        "Split": name,
        "Fake (0)": counts[0] if 0 in counts else 0,
        "Real (1)": counts[1] if 1 in counts else 0,
        "Total": len(subset)
    }

split_distribution = pd.DataFrame([
    get_distribution("Train", train_df_split),
    get_distribution("Validation", val_df),
    get_distribution("Test", test_df)
])

print("\n=== Split-wise Distribution ===")
print(split_distribution)

import matplotlib.pyplot as plt
import seaborn as sns

# --- Full dataset distribution ---
class_counts = train_df['label'].value_counts().sort_index()

plt.figure(figsize=(6,5))
sns.barplot(x=["Fake (0)", "Real (1)"], y=class_counts.values, palette="Blues")
plt.title("Class Distribution (Full Dataset)")
plt.ylabel("Count")
plt.xlabel("Class")
plt.grid(axis="y", linestyle="--", alpha=0.6)
for i, v in enumerate(class_counts.values):
    plt.text(i, v + 200, str(v), ha='center', fontweight='bold')
plt.show()

# --- Split-wise distribution ---
splits = ["Train", "Validation", "Test"]
split_counts = [
    train_df_split['label'].value_counts().sort_index(),
    val_df['label'].value_counts().sort_index(),
    test_df['label'].value_counts().sort_index()
]

split_df = pd.DataFrame(split_counts, index=splits)
split_df.plot(kind="bar", stacked=False, figsize=(7,5), color=["skyblue","orange"])

plt.title("Split-wise Class Distribution")
plt.ylabel("Count")
plt.xlabel("Dataset Split")
plt.xticks(rotation=0)
plt.legend(["Fake (0)", "Real (1)"])
plt.grid(axis="y", linestyle="--", alpha=0.6)
plt.show()

"""# ***Prediction***"""

import glob
import os
import numpy as np
import torch

# Path to all your SEMrush/MOZ CSVs
input_folder = "/kaggle/input/backlinks-data-cleaned"   # <-- change dataset name
csv_files = glob.glob(os.path.join(input_folder, "*.csv"))

print(f"📂 Found {len(csv_files)} CSV files")

results_summary = []

for file in csv_files:
    # === 1. Load CSV ===
    df = pd.read_csv(file)
    print(f"\nProcessing file: {os.path.basename(file)} | Rows: {len(df)}")

    # === 2. Ensure "text" column ===
    if "text" not in df.columns:
        for col in ["content","body","article","headline","title"]:
            if col in df.columns:
                df = df.rename(columns={col: "text"})
                print(f"ℹ️ Renamed column '{col}' → 'text'")
                break

    texts = df["text"].astype(str).tolist()

    # === 3. Create Dataset ===
    class NewsDataset(torch.utils.data.Dataset):
        def __init__(self, texts, tokenizer, max_length=256):
            self.enc = tokenizer(
                texts, truncation=True, padding="max_length", max_length=max_length
            )

        def __len__(self):
            return len(self.enc["input_ids"])

        def __getitem__(self, idx):
            return {k: torch.tensor(v[idx]) for k, v in self.enc.items()}

    pred_dataset = NewsDataset(texts, tokenizer)

    # === 4. Run Predictions ===
    predictions = trainer.predict(pred_dataset)
    y_pred = np.argmax(predictions.predictions, axis=1)
    y_probs = torch.nn.functional.softmax(torch.tensor(predictions.predictions), dim=1)[:,1].numpy()

    # === 5. Attach Predictions ===
    df["Prediction"] = y_pred
    df["Prediction"] = df["Prediction"].map({0: "Fake", 1: "Real"})
    df["Confidence"] = y_probs

    # === 6. Save File to /kaggle/working ===
    output_file = f"/kaggle/working/{os.path.basename(file).replace('.csv','_pred_2.csv')}"
    df.to_csv(output_file, index=False)
    print(f"✅ Saved predictions to {output_file}")

    # === 7. Collect Summary ===
    results_summary.append({
        "File": os.path.basename(file),
        "Total Rows": len(df),
        "Real Count": (df["Prediction"]=="Real").sum(),
        "Fake Count": (df["Prediction"]=="Fake").sum(),
        "Avg Confidence": df["Confidence"].mean()
    })

# === 8. Save Summary Report ===
summary_df = pd.DataFrame(results_summary)
summary_path = "/kaggle/working/predictions_summary.csv"
summary_df.to_csv(summary_path, index=False)

print("\n📊 Summary of all files saved to:", summary_path)
summary_df

import matplotlib.pyplot as plt
import seaborn as sns

# Load the summary from previous step
summary_df = pd.read_csv("/kaggle/working/predictions_summary.csv")

# 1️⃣ Bar chart: Fake vs Real per file
plt.figure(figsize=(10,6))
sns.barplot(data=summary_df.melt(id_vars="File", value_vars=["Real Count","Fake Count"]),
            x="File", y="value", hue="variable")
plt.title("Fake vs Real Predictions per File")
plt.ylabel("Article Count")
plt.xticks(rotation=45, ha="right")
plt.legend(title="Prediction")
plt.tight_layout()
plt.show()

# 2️⃣ Bar chart: Average Confidence per file
plt.figure(figsize=(10,6))
sns.barplot(data=summary_df, x="File", y="Avg Confidence", palette="viridis")
plt.title("Average Confidence per File")
plt.ylabel("Confidence (0 = Fake, 1 = Real)")
plt.xticks(rotation=45, ha="right")
plt.tight_layout()
plt.show()

# 3️⃣ Pie chart: Overall Fake vs Real across all files
total_fake = summary_df["Fake Count"].sum()
total_real = summary_df["Real Count"].sum()

plt.figure(figsize=(6,6))
plt.pie([total_real, total_fake],
        labels=["Real","Fake"],
        autopct="%1.1f%%",
        colors=["green","red"],
        startangle=140)
plt.title("Overall Prediction Distribution Across All Files")
plt.show()